{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        #print(n_observations, n_actions)\n",
    "        self.layer1 = nn.Linear(n_observations, 256) #nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(256, 256) #nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(256, n_actions) #nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import *  \n",
    "import random\n",
    "\n",
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128 #256 #128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = len(ACTION_SPACE) #env.action_space.n\n",
    "# Get the number of state observations\n",
    "grid = get_start_grid()\n",
    "map_x_len = len(grid[0])\n",
    "map_y_len = len(grid)\n",
    "\n",
    "#state, info = env.reset()\n",
    "n_observations = map_x_len * map_y_len * 4 #len(state) # 4 matrices representing entire game\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else: # Uniform random selection of action\n",
    "        randAction = random.randint(0, n_actions-1)\n",
    "        return torch.tensor([[randAction]], device=device, dtype=torch.long) #torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulated Rewards') #plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 of 50000 (0.20%), ETA: 7178.26 seconds, last score: -100.0, best score: 0.0\n",
      "Epoch 199 of 50000 (0.40%), ETA: 7740.32 seconds, last score: -60.0, best score: 10.0\n",
      "Epoch 299 of 50000 (0.60%), ETA: 7488.45 seconds, last score: -100.0, best score: 10.0\n",
      "Epoch 399 of 50000 (0.80%), ETA: 7824.06 seconds, last score: -10.0, best score: 10.0\n",
      "Epoch 499 of 50000 (1.00%), ETA: 8677.28 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 599 of 50000 (1.20%), ETA: 7711.99 seconds, last score: 40.0, best score: 40.0\n",
      "Epoch 699 of 50000 (1.40%), ETA: 8449.70 seconds, last score: -40.0, best score: 40.0\n",
      "Epoch 799 of 50000 (1.60%), ETA: 8831.58 seconds, last score: -100.0, best score: 40.0\n",
      "Epoch 899 of 50000 (1.80%), ETA: 8925.34 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 999 of 50000 (2.00%), ETA: 7003.47 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 1099 of 50000 (2.20%), ETA: 6897.49 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 1199 of 50000 (2.40%), ETA: 9802.90 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 1299 of 50000 (2.60%), ETA: 9488.17 seconds, last score: -40.0, best score: 40.0\n",
      "Epoch 1399 of 50000 (2.80%), ETA: 10568.53 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 1499 of 50000 (3.00%), ETA: 9149.96 seconds, last score: -45.0, best score: 40.0\n",
      "Epoch 1599 of 50000 (3.20%), ETA: 7740.77 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 1699 of 50000 (3.40%), ETA: 8465.72 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 1799 of 50000 (3.60%), ETA: 7950.76 seconds, last score: -30.0, best score: 40.0\n",
      "Epoch 1899 of 50000 (3.80%), ETA: 6477.76 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 1999 of 50000 (4.00%), ETA: 6398.29 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 2099 of 50000 (4.20%), ETA: 6057.90 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 2199 of 50000 (4.40%), ETA: 6860.40 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 2299 of 50000 (4.60%), ETA: 7172.08 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 2399 of 50000 (4.80%), ETA: 7082.32 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 2499 of 50000 (5.00%), ETA: 7282.14 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 2599 of 50000 (5.20%), ETA: 7987.07 seconds, last score: -100.0, best score: 40.0\n",
      "Epoch 2699 of 50000 (5.40%), ETA: 5689.13 seconds, last score: -25.0, best score: 40.0\n",
      "Epoch 2799 of 50000 (5.60%), ETA: 7117.20 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 2899 of 50000 (5.80%), ETA: 7797.81 seconds, last score: -50.0, best score: 40.0\n",
      "Epoch 2999 of 50000 (6.00%), ETA: 9068.84 seconds, last score: -40.0, best score: 40.0\n",
      "Epoch 3099 of 50000 (6.20%), ETA: 6755.15 seconds, last score: -45.0, best score: 40.0\n",
      "Epoch 3199 of 50000 (6.40%), ETA: 8115.06 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 3299 of 50000 (6.60%), ETA: 7391.13 seconds, last score: -60.0, best score: 40.0\n",
      "Epoch 3399 of 50000 (6.80%), ETA: 6775.32 seconds, last score: -45.0, best score: 40.0\n",
      "Epoch 3499 of 50000 (7.00%), ETA: 8359.48 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 3599 of 50000 (7.20%), ETA: 7819.03 seconds, last score: -45.0, best score: 40.0\n",
      "Epoch 3699 of 50000 (7.40%), ETA: 7820.93 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 3799 of 50000 (7.60%), ETA: 8631.04 seconds, last score: -10.0, best score: 40.0\n",
      "Epoch 3899 of 50000 (7.80%), ETA: 7865.29 seconds, last score: -215.0, best score: 40.0\n",
      "Epoch 3999 of 50000 (8.00%), ETA: 8443.25 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 4099 of 50000 (8.20%), ETA: 8700.53 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 4199 of 50000 (8.40%), ETA: 7687.47 seconds, last score: -45.0, best score: 40.0\n",
      "Epoch 4299 of 50000 (8.60%), ETA: 7755.46 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 4399 of 50000 (8.80%), ETA: 7929.56 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 4499 of 50000 (9.00%), ETA: 8590.13 seconds, last score: -50.0, best score: 40.0\n",
      "Epoch 4599 of 50000 (9.20%), ETA: 8577.85 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 4699 of 50000 (9.40%), ETA: 7646.81 seconds, last score: -45.0, best score: 40.0\n",
      "Epoch 4799 of 50000 (9.60%), ETA: 7608.01 seconds, last score: -70.0, best score: 40.0\n",
      "Epoch 4899 of 50000 (9.80%), ETA: 7801.12 seconds, last score: -180.0, best score: 40.0\n",
      "Epoch 4999 of 50000 (10.00%), ETA: 8990.97 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 5099 of 50000 (10.20%), ETA: 8288.72 seconds, last score: -35.0, best score: 40.0\n",
      "Epoch 5199 of 50000 (10.40%), ETA: 7882.51 seconds, last score: -190.0, best score: 40.0\n",
      "Epoch 5299 of 50000 (10.60%), ETA: 8091.33 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 5399 of 50000 (10.80%), ETA: 7785.33 seconds, last score: -30.0, best score: 40.0\n",
      "Epoch 5499 of 50000 (11.00%), ETA: 8172.61 seconds, last score: -45.0, best score: 40.0\n",
      "Epoch 5599 of 50000 (11.20%), ETA: 6839.53 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 5699 of 50000 (11.40%), ETA: 8816.92 seconds, last score: -205.0, best score: 40.0\n",
      "Epoch 5799 of 50000 (11.60%), ETA: 7603.68 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 5899 of 50000 (11.80%), ETA: 7593.53 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 5999 of 50000 (12.00%), ETA: 7237.28 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 6099 of 50000 (12.20%), ETA: 8763.96 seconds, last score: -50.0, best score: 40.0\n",
      "Epoch 6199 of 50000 (12.40%), ETA: 8311.46 seconds, last score: -20.0, best score: 40.0\n",
      "Epoch 6299 of 50000 (12.60%), ETA: 9126.08 seconds, last score: -65.0, best score: 40.0\n",
      "Epoch 6399 of 50000 (12.80%), ETA: 9108.03 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 6499 of 50000 (13.00%), ETA: 7726.43 seconds, last score: -60.0, best score: 40.0\n",
      "Epoch 6599 of 50000 (13.20%), ETA: 8267.67 seconds, last score: -50.0, best score: 40.0\n",
      "Epoch 6699 of 50000 (13.40%), ETA: 6274.75 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 6799 of 50000 (13.60%), ETA: 6776.94 seconds, last score: -30.0, best score: 40.0\n",
      "Epoch 6899 of 50000 (13.80%), ETA: 8524.30 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 6999 of 50000 (14.00%), ETA: 7470.56 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 7099 of 50000 (14.20%), ETA: 6164.87 seconds, last score: -100.0, best score: 40.0\n",
      "Epoch 7199 of 50000 (14.40%), ETA: 2713.23 seconds, last score: 0.0, best score: 40.0\n",
      "Epoch 7299 of 50000 (14.60%), ETA: 1271.42 seconds, last score: -90.0, best score: 40.0\n",
      "Epoch 7399 of 50000 (14.80%), ETA: 996.86 seconds, last score: -100.0, best score: 50.0\n",
      "Epoch 7499 of 50000 (15.00%), ETA: 1014.29 seconds, last score: -15.0, best score: 50.0\n",
      "Epoch 7599 of 50000 (15.20%), ETA: 952.96 seconds, last score: 0.0, best score: 50.0\n",
      "Epoch 7699 of 50000 (15.40%), ETA: 983.29 seconds, last score: -90.0, best score: 50.0\n",
      "Epoch 7799 of 50000 (15.60%), ETA: 804.98 seconds, last score: -15.0, best score: 50.0\n",
      "Epoch 7899 of 50000 (15.80%), ETA: 795.92 seconds, last score: 0.0, best score: 50.0\n",
      "Epoch 7999 of 50000 (16.00%), ETA: 1138.86 seconds, last score: -150.0, best score: 50.0\n",
      "Epoch 8099 of 50000 (16.20%), ETA: 1293.06 seconds, last score: 10.0, best score: 50.0\n",
      "Epoch 8199 of 50000 (16.40%), ETA: 1788.46 seconds, last score: 20.0, best score: 50.0\n",
      "Epoch 8299 of 50000 (16.60%), ETA: 781.69 seconds, last score: 10.0, best score: 50.0\n",
      "Epoch 8399 of 50000 (16.80%), ETA: 1121.35 seconds, last score: 0.0, best score: 50.0\n",
      "Epoch 8499 of 50000 (17.00%), ETA: 1723.33 seconds, last score: -180.0, best score: 50.0\n",
      "Epoch 8599 of 50000 (17.20%), ETA: 7809.47 seconds, last score: 0.0, best score: 50.0\n",
      "Epoch 8699 of 50000 (17.40%), ETA: 6491.69 seconds, last score: -100.0, best score: 50.0\n",
      "Epoch 8799 of 50000 (17.60%), ETA: 6978.83 seconds, last score: 0.0, best score: 50.0\n",
      "Epoch 8899 of 50000 (17.80%), ETA: 7420.99 seconds, last score: 0.0, best score: 50.0\n",
      "Epoch 8999 of 50000 (18.00%), ETA: 6545.40 seconds, last score: 0.0, best score: 50.0\n",
      "Epoch 9099 of 50000 (18.20%), ETA: 7841.74 seconds, last score: 0.0, best score: 50.0\n",
      "Epoch 9199 of 50000 (18.40%), ETA: 6887.00 seconds, last score: 0.0, best score: 50.0\n",
      "Epoch 9299 of 50000 (18.60%), ETA: 7046.16 seconds, last score: -55.0, best score: 50.0\n",
      "Epoch 9399 of 50000 (18.80%), ETA: 7330.31 seconds, last score: 0.0, best score: 50.0\n",
      "Epoch 9499 of 50000 (19.00%), ETA: 7532.78 seconds, last score: 0.0, best score: 50.0\n",
      "Epoch 9599 of 50000 (19.20%), ETA: 6919.88 seconds, last score: -100.0, best score: 50.0\n",
      "Epoch 9699 of 50000 (19.40%), ETA: 7039.07 seconds, last score: 0.0, best score: 50.0\n",
      "Epoch 9799 of 50000 (19.60%), ETA: 7278.79 seconds, last score: 0.0, best score: 50.0\n",
      "Epoch 9899 of 50000 (19.80%), ETA: 7122.54 seconds, last score: 0.0, best score: 50.0\n",
      "Epoch 9999 of 50000 (20.00%), ETA: 7154.58 seconds, last score: 0.0, best score: 50.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DQLearnPlayer.__init__() missing 1 required positional argument: 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 53\u001b[0m\n\u001b[0;32m     49\u001b[0m     policy_net_copy2 \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(policy_net)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i_episode \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m:\n\u001b[1;32m---> 53\u001b[0m     walker \u001b[38;5;241m=\u001b[39m DQLearnPlayer(policy_net_copy, device)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     walker \u001b[38;5;241m=\u001b[39m WalkerPlayer()\n",
      "\u001b[1;31mTypeError\u001b[0m: DQLearnPlayer.__init__() missing 1 required positional argument: 'device'"
     ]
    }
   ],
   "source": [
    "import game\n",
    "from environment import DQEnv, WalkerPlayer, DQLearnPlayer\n",
    "import datetime\n",
    "import torch.jit as jit\n",
    "import copy\n",
    "\n",
    "import importlib\n",
    "importlib.reload(game)\n",
    "\n",
    "max_steps = 1000\n",
    "\n",
    "num_episodes = 0\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes = 50000 #10000 #600\n",
    "#else:\n",
    "#    num_episodes = 50\n",
    "\n",
    "timelast = datetime.datetime.now()\n",
    "last_score = 0\n",
    "best_score = -float('inf')\n",
    "\n",
    "policy_net_copy = None\n",
    "policy_net_copy2 = None\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    if (i_episode + 1) % 100 == 0:\n",
    "        time_per_epoch = (datetime.datetime.now() - timelast).total_seconds()/100\n",
    "        print(f'Epoch {i_episode} of {num_episodes} ({i_episode/num_episodes*100:.2f}%),'\n",
    "              f' ETA: {(num_episodes-i_episode)*time_per_epoch:.2f} seconds,'\n",
    "              f' last score: {last_score}, best score: {best_score}')\n",
    "        timelast = datetime.datetime.now()\n",
    "\n",
    "    # Initialize the environment and get its state\n",
    "    #state, info = env.reset()\n",
    "    game.grid = game.get_start_grid()\n",
    "    game.alive_players = []\n",
    "    game.dead_players = []\n",
    "    game.global_bombs = set()\n",
    "    p1 = game.Player('DQL')\n",
    "    env = DQEnv(p1) #QEnv(p1)\n",
    "\n",
    "    walker = None\n",
    "    walker1 = None\n",
    "    walker2 = WalkerPlayer()\n",
    "\n",
    "    if i_episode % 10000 == 0 and (i_episode / 10000) % 2 != 0 :\n",
    "        policy_net_copy = copy.deepcopy(policy_net)\n",
    "    if i_episode % 10000 == 0 and (i_episode / 10000) % 2 == 0:\n",
    "        policy_net_copy2 = copy.deepcopy(policy_net)\n",
    "\n",
    "\n",
    "    if i_episode >= 10000:\n",
    "        walker = DQLearnPlayer(policy_net_copy, device)\n",
    "    else:\n",
    "        walker = WalkerPlayer()\n",
    "    if i_episode >= 20000:\n",
    "        walker1 = DQLearnPlayer(policy_net_copy2, device)\n",
    "    else:\n",
    "        walker1 = WalkerPlayer()\n",
    "\n",
    "\n",
    "    walker.start()\n",
    "    walker1.start()\n",
    "    walker2.start()\n",
    "\n",
    "    players_grid, power_up_grid, blocks_grid, bomb_grid = p1.get_self_grid()\n",
    "\n",
    "    state = np.concatenate((players_grid.flatten(), power_up_grid.flatten(), blocks_grid.flatten(), bomb_grid.flatten()))\n",
    "    #print(state.shape)\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    #print(state.shape)\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(max_steps): #for t in count():\n",
    "        action = select_action(state)\n",
    "        #observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        #done = terminated or truncated\n",
    "\n",
    "        #if done:\n",
    "        next_state = None\n",
    "        if not done:\n",
    "            players_grid2, power_up_grid2, blocks_grid2, bomb_grid2 = p1.get_self_grid()\n",
    "            observation = np.concatenate((players_grid2.flatten(), power_up_grid2.flatten(), blocks_grid2.flatten(), bomb_grid2.flatten()))\n",
    "            #observation = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "\n",
    "        if done or len(game.alive_players) < 2:\n",
    "            last_score = p1.get_score()\n",
    "            if last_score > best_score:\n",
    "                best_score = last_score\n",
    "            \"\"\"\n",
    "            env.reset()\n",
    "            walker.stop()\n",
    "            walker1.stop()\n",
    "            walker2.stop()\n",
    "            \"\"\"\n",
    "            break\n",
    "\n",
    "\n",
    "        if done or t == max_steps - 1:\n",
    "            episode_durations.append(total_reward)#t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    \n",
    "    env.reset()\n",
    "    walker.stop()\n",
    "    walker1.stop()\n",
    "    walker2.stop()\n",
    "    \n",
    "\n",
    "jit.save(torch.jit.script(policy_net), \"DQL_model\")\n",
    "#model = jit.load(\"DQL_model\")\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nplt.plot(episode_durations)\\nplt.title('Result')\\nplt.xlabel('Episode')\\nplt.ylabel('Cumulated Rewards') #plt.ylabel('Duration')\\nplt.savefig('DQL-rewardsPlot.png')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from game import * \n",
    "import numpy as np \n",
    "import random\n",
    "import game\n",
    "from environment import DQEnv, WalkerPlayer, DQLearnPlayer\n",
    "import datetime\n",
    "import torch.jit as jit\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "\n",
    "#plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "\n",
    "################################################## Replay memory\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "\n",
    "################################################## Q-network\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        #print(n_observations, n_actions)\n",
    "        self.layer1 = nn.Linear(n_observations, 256) #nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(256, 256) #nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(256, n_actions) #nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    \n",
    "################################################## Training\n",
    "\n",
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 256 #128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = len(game.ACTION_SPACE) #env.action_space.n\n",
    "# Get the number of state observations\n",
    "grid = game.get_start_grid()\n",
    "map_x_len = len(grid[0])\n",
    "map_y_len = len(grid)\n",
    "\n",
    "#state, info = env.reset()\n",
    "n_observations = (map_x_len * map_y_len * 4) + 1 #len(state) # 4 matrices representing entire game\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else: # Uniform random selection of action\n",
    "        randAction = random.randint(0, n_actions-1)\n",
    "        return torch.tensor([[randAction]], device=device, dtype=torch.long) #torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    #plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    #if show_result:\n",
    "    plt.plot(durations_t.numpy())\n",
    "    plt.title('Result')\n",
    "    #else:\n",
    "    #    plt.clf()\n",
    "    #    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulated Rewards') #plt.ylabel('Duration')\n",
    "    # Take 100 episode averages and plot them too\n",
    "    #if len(durations_t) >= 100:\n",
    "    #    means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "    #    means = torch.cat((torch.zeros(99), means))\n",
    "    #    plt.plot(means.numpy())\n",
    "\n",
    "    \n",
    "    plt.savefig('DQL-rewardsPlot.png') \n",
    "\n",
    "    #plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    #if is_ipython:\n",
    "        #if not show_result:\n",
    "        #    display.display(plt.gcf())\n",
    "        #    display.clear_output(wait=True)\n",
    "        #else:\n",
    "        #   display.display(plt.gcf())\n",
    "\n",
    "################################################## Training optimizer\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "################################################## Training loop\n",
    "demonstration = False\n",
    "if demonstration:\n",
    "    game.HEADLESS = False\n",
    "    game.TIME_CONST = 0.25\n",
    "else:\n",
    "    game.HEADLESS = True\n",
    "    game.TIME_CONST = 0.001\n",
    "\n",
    "max_steps = 1000 #Nr of actions before ending game if the player did not win/die\n",
    "\n",
    "num_episodes = 0\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes = 5#50000 #10000 #600\n",
    "#else:\n",
    "#    num_episodes = 50\n",
    "\n",
    "timelast = datetime.datetime.now()\n",
    "last_score = 0\n",
    "best_score = -float('inf')\n",
    "\n",
    "policy_net_copy = None\n",
    "policy_net_copy2 = None\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    if (i_episode + 1) % 100 == 0:\n",
    "        time_per_epoch = (datetime.datetime.now() - timelast).total_seconds()/100\n",
    "        print(f'Epoch {i_episode} of {num_episodes} ({i_episode/num_episodes*100:.2f}%),'\n",
    "              f' ETA: {(num_episodes-i_episode)*time_per_epoch:.2f} seconds,'\n",
    "              f' last score: {last_score}, best score: {best_score}')\n",
    "        timelast = datetime.datetime.now()\n",
    "\n",
    "    # Initialize the environment and get its state\n",
    "    #state, info = env.reset()\n",
    "    game.grid = game.get_start_grid()\n",
    "    game.alive_players = []\n",
    "    game.dead_players = []\n",
    "    game.global_bombs = set()\n",
    "    p1 = game.Player('DQL')\n",
    "    env = DQEnv(p1) #QEnv(p1)\n",
    "\n",
    "    walker = None\n",
    "    walker1 = None\n",
    "    walker2 = WalkerPlayer()\n",
    "\n",
    "    if i_episode % 10000 == 0 and (i_episode / 10000) % 2 != 0 :\n",
    "        policy_net_copy = copy.deepcopy(policy_net)\n",
    "    if i_episode % 10000 == 0 and (i_episode / 10000) % 2 == 0:\n",
    "        policy_net_copy2 = copy.deepcopy(policy_net)\n",
    "\n",
    "\n",
    "    if i_episode >= 10000:\n",
    "        walker = DQLearnPlayer(policy_net_copy, device)\n",
    "    else:\n",
    "        walker = WalkerPlayer()\n",
    "    if i_episode >= 20000:\n",
    "        walker1 = DQLearnPlayer(policy_net_copy2, device)\n",
    "    else:\n",
    "        walker1 = WalkerPlayer()\n",
    "\n",
    "\n",
    "    walker.start()\n",
    "    walker1.start()\n",
    "    walker2.start()\n",
    "\n",
    "    players_grid, power_up_grid, blocks_grid, bomb_grid = p1.get_self_grid()\n",
    "    nrBombs = np.array([p1.get_bombs()], dtype=np.uint8)\n",
    "    state = np.concatenate((players_grid.flatten(), power_up_grid.flatten(), blocks_grid.flatten(), bomb_grid.flatten(), nrBombs))\n",
    "    #print(state.shape)\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    #print(state.shape)\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(max_steps): #for t in count():\n",
    "        action = select_action(state)\n",
    "        #observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        #done = terminated or truncated\n",
    "\n",
    "        #if done:\n",
    "        next_state = None\n",
    "        if not done:\n",
    "            players_grid2, power_up_grid2, blocks_grid2, bomb_grid2 = p1.get_self_grid()\n",
    "            nrBombs2 = np.array([p1.get_bombs()], dtype=np.uint8)\n",
    "            observation = np.concatenate((players_grid2.flatten(), power_up_grid2.flatten(), blocks_grid2.flatten(), bomb_grid2.flatten(), nrBombs2))\n",
    "            #observation = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "\n",
    "        if done or len(game.alive_players) < 2 or t == max_steps - 1:\n",
    "            last_score = p1.get_score()\n",
    "            if last_score > best_score:\n",
    "                best_score = last_score\n",
    "            \"\"\"\n",
    "            env.reset()\n",
    "            walker.stop()\n",
    "            walker1.stop()\n",
    "            walker2.stop()\n",
    "            \"\"\"\n",
    "            episode_durations.append(total_reward)\n",
    "            break\n",
    "\n",
    "        \"\"\"\n",
    "        if done or t == max_steps - 1:\n",
    "            episode_durations.append(total_reward)#t + 1)\n",
    "            #plot_durations()\n",
    "            break\n",
    "        \"\"\"\n",
    "    \n",
    "    env.reset()\n",
    "    walker.stop()\n",
    "    walker1.stop()\n",
    "    walker2.stop()\n",
    "    \n",
    "\n",
    "jit.save(torch.jit.script(policy_net), \"DQL_model\")\n",
    "#model = jit.load(\"DQL_model\")\n",
    "\n",
    "print('Complete')\n",
    "#plot_durations(show_result=True)\n",
    "#plt.ioff()\n",
    "\n",
    "#plt.show()\n",
    "\"\"\"\n",
    "plt.plot(episode_durations)\n",
    "plt.title('Result')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulated Rewards') #plt.ylabel('Duration')\n",
    "plt.savefig('DQL-rewardsPlot.png')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08464787338161128\n",
      "0.04076220397836621 -3.2\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "EPS_END = 0.05\n",
    "EPS_START = 0.9\n",
    "EPS_DECAY = 500000\n",
    "steps_done = 80000 * 20\n",
    "\n",
    "eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "\n",
    "print(eps_threshold)\n",
    "\n",
    "print(math.exp(-1. * steps_done / EPS_DECAY), -1. * steps_done / EPS_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################## Training loop\n",
    "demonstration = False\n",
    "if demonstration:\n",
    "    game.HEADLESS = False\n",
    "    game.TIME_CONST = 0.25\n",
    "else:\n",
    "    game.HEADLESS = True\n",
    "    game.TIME_CONST = 0.001\n",
    "\n",
    "average_of_100 = deque([0]*100,maxlen=100)\n",
    "\n",
    "max_steps = 1000 #Nr of actions before ending game if the player did not win/die\n",
    "\n",
    "num_episodes = 0\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes = 100000 #50000 #10000 #600\n",
    "\n",
    "timelast = datetime.datetime.now()\n",
    "last_score = 0\n",
    "best_score = -float('inf')\n",
    "\n",
    "policy_net_copy = None\n",
    "policy_net_copy2 = None\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    average_of_100.appendleft(last_score)\n",
    "    if (i_episode + 1) % 100 == 0:\n",
    "        time_per_epoch = (datetime.datetime.now() - timelast).total_seconds()/100\n",
    "        print(f'Epoch {i_episode} of {num_episodes} ({i_episode/num_episodes*100:.2f}%),'\n",
    "              f' ETA: {(num_episodes-i_episode)*time_per_epoch:.2f} seconds,'\n",
    "              f' last score: {last_score}, best score: {best_score}'\n",
    "              f' average of last 100: {sum(average_of_100)/len(average_of_100):.2f}')\n",
    "        timelast = datetime.datetime.now()\n",
    "\n",
    "    # Initialize the environment and get its state\n",
    "    game.grid = game.get_start_grid()\n",
    "    game.alive_players = []\n",
    "    game.dead_players = []\n",
    "    game.global_bombs = set()\n",
    "    p1 = game.Player('DQL')\n",
    "    env = DQEnv(p1)\n",
    "\n",
    "    walker = None\n",
    "    walker1 = None\n",
    "    walker2 = WalkerPlayer()\n",
    "    walker = WalkerPlayer()\n",
    "    walker1 = WalkerPlayer()\n",
    "\n",
    "\n",
    "    walker.start()\n",
    "    walker1.start()\n",
    "    walker2.start()\n",
    "\n",
    "    players_grid, power_up_grid, blocks_grid, bomb_grid = p1.get_self_grid()\n",
    "    nrBombs = np.array([p1.get_bombs()], dtype=np.uint8)\n",
    "    state = np.concatenate((players_grid.flatten(), power_up_grid.flatten(), blocks_grid.flatten(), bomb_grid.flatten(), nrBombs))\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        action = select_action(state)\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        next_state = None\n",
    "        if not done:\n",
    "            players_grid2, power_up_grid2, blocks_grid2, bomb_grid2 = p1.get_self_grid()\n",
    "            nrBombs2 = np.array([p1.get_bombs()], dtype=np.uint8)\n",
    "            observation = np.concatenate((players_grid2.flatten(), power_up_grid2.flatten(), blocks_grid2.flatten(), bomb_grid2.flatten(), nrBombs2))\n",
    "\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "\n",
    "        if done or len(game.alive_players) < 2 or t == max_steps - 1:\n",
    "            last_score = p1.get_score()\n",
    "            if last_score > best_score:\n",
    "                best_score = last_score\n",
    "            episode_durations.append(total_reward)\n",
    "            break\n",
    "    \n",
    "    env.reset()\n",
    "    walker.stop()\n",
    "    walker1.stop()\n",
    "    walker2.stop()\n",
    "    \n",
    "\n",
    "jit.save(torch.jit.script(policy_net), \"DQL_model\")\n",
    "\n",
    "print('Complete')\n",
    "\n",
    "np.savetxt(\"DQL_rewardsList.txt\", np.array(episode_durations), delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
